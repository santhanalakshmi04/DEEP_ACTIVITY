{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0fWaNpUyNvXlC0avxuguz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santhanalakshmi04/DEEP_ACTIVITY/blob/main/DEEP_ACT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NAME : SANTHANA LAKSHMI K\n",
        "REG NO : 212222240091"
      ],
      "metadata": {
        "id": "_0JkbUCaneAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide a brief explanation of the architecture and working process of a GAN, followed by a simple Python code example using PyTorch on the MNIST dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "1VCH2gImnQWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture and Working Process of a GAN\n",
        "A Generative Adversarial Network (GAN) is a framework composed of two neural networks, trained simultaneously through adversarial processes:\n",
        "\n",
        "Generator (G):\n",
        "\n",
        "Takes random noise as input and generates data resembling the real dataset.\n",
        "\n",
        "Its goal is to fool the Discriminator into classifying its output as real.\n",
        "\n",
        "Discriminator (D):\n",
        "\n",
        "Takes data samples and tries to distinguish between real data (from the dataset) and fake data (from the Generator).\n",
        "\n",
        "Its goal is to correctly classify real vs. fake inputs.\n",
        "\n",
        "Training Process:\n",
        "\n",
        "The Generator and Discriminator are trained together in a loop.\n",
        "\n",
        "Discriminator is trained to maximize the probability of assigning the correct label to both real and fake samples.\n",
        "\n",
        "Generator is trained to minimize the probability that the Discriminator correctly identifies its outputs as fake.\n",
        "\n",
        "This leads to a minimax game where the Generator improves to produce more realistic data, and the Discriminator becomes better at detection."
      ],
      "metadata": {
        "id": "-RiXHdetnW_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "JWePblEKcy0u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "latent_size = 64\n",
        "hidden_size = 256\n",
        "image_size = 28 * 28\n",
        "batch_size = 100\n",
        "num_epochs = 50\n",
        "learning_rate = 0.0002"
      ],
      "metadata": {
        "id": "ubuaOW5qc1kt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST data loader\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "tC9mZZGmc56K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator\n",
        "D = nn.Sequential(\n",
        "    nn.Linear(image_size, hidden_size),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(hidden_size, 1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "N8Vxqa56c8EZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator\n",
        "G = nn.Sequential(\n",
        "    nn.Linear(latent_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, image_size),\n",
        "    nn.Tanh()\n",
        ")"
      ],
      "metadata": {
        "id": "9_ut3z9uc-bm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "d_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate)\n",
        "g_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Q3_YFHffdAXL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "D, G = D.to(device), G.to(device)"
      ],
      "metadata": {
        "id": "A8Gsgy58dCaO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, _) in enumerate(dataloader):\n",
        "        # Flatten images\n",
        "        images = images.view(batch_size, -1).to(device)\n",
        "\n",
        "        # Labels\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        outputs = D(images)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "        z = torch.randn(batch_size, latent_size).to(device)\n",
        "        fake_images = G(z)\n",
        "        outputs = D(fake_images.detach())\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_optimizer.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # Train Generator\n",
        "        z = torch.randn(batch_size, latent_size).to(device)\n",
        "        fake_images = G(z)\n",
        "        outputs = D(fake_images)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        g_optimizer.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdMgdmNudEmK",
        "outputId": "d83d99b8-f43f-4271-e21d-32f0fb7d856e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], d_loss: 1.5046, g_loss: 0.6138\n",
            "Epoch [2/50], d_loss: 0.9335, g_loss: 1.1116\n",
            "Epoch [3/50], d_loss: 0.7692, g_loss: 1.2611\n",
            "Epoch [4/50], d_loss: 0.9564, g_loss: 1.2394\n",
            "Epoch [5/50], d_loss: 1.0760, g_loss: 1.1503\n",
            "Epoch [6/50], d_loss: 0.8248, g_loss: 1.2817\n",
            "Epoch [7/50], d_loss: 1.1150, g_loss: 1.0391\n",
            "Epoch [8/50], d_loss: 0.6740, g_loss: 1.5675\n",
            "Epoch [9/50], d_loss: 1.0489, g_loss: 1.2985\n",
            "Epoch [10/50], d_loss: 0.5335, g_loss: 1.6943\n",
            "Epoch [11/50], d_loss: 0.9098, g_loss: 1.2413\n",
            "Epoch [12/50], d_loss: 0.9682, g_loss: 1.1007\n",
            "Epoch [13/50], d_loss: 1.0468, g_loss: 1.7855\n",
            "Epoch [14/50], d_loss: 1.3743, g_loss: 0.9475\n",
            "Epoch [15/50], d_loss: 1.1900, g_loss: 0.9802\n",
            "Epoch [16/50], d_loss: 0.9104, g_loss: 1.6892\n",
            "Epoch [17/50], d_loss: 0.6465, g_loss: 2.0010\n",
            "Epoch [18/50], d_loss: 1.5691, g_loss: 0.9755\n",
            "Epoch [19/50], d_loss: 1.0015, g_loss: 1.1269\n",
            "Epoch [20/50], d_loss: 1.2013, g_loss: 1.2140\n",
            "Epoch [21/50], d_loss: 0.7100, g_loss: 1.6279\n",
            "Epoch [22/50], d_loss: 0.9273, g_loss: 1.2452\n",
            "Epoch [23/50], d_loss: 1.3458, g_loss: 1.0286\n",
            "Epoch [24/50], d_loss: 0.8878, g_loss: 1.2220\n",
            "Epoch [25/50], d_loss: 1.1200, g_loss: 1.1957\n",
            "Epoch [26/50], d_loss: 0.9024, g_loss: 1.5726\n",
            "Epoch [27/50], d_loss: 0.4859, g_loss: 1.9877\n",
            "Epoch [28/50], d_loss: 0.6246, g_loss: 2.5444\n",
            "Epoch [29/50], d_loss: 0.4885, g_loss: 2.1518\n",
            "Epoch [30/50], d_loss: 0.8556, g_loss: 1.4825\n",
            "Epoch [31/50], d_loss: 0.7087, g_loss: 1.9664\n",
            "Epoch [32/50], d_loss: 0.7699, g_loss: 1.5965\n",
            "Epoch [33/50], d_loss: 0.9866, g_loss: 1.4134\n",
            "Epoch [34/50], d_loss: 1.0419, g_loss: 1.6603\n",
            "Epoch [35/50], d_loss: 1.0079, g_loss: 1.3451\n",
            "Epoch [36/50], d_loss: 0.8724, g_loss: 1.7320\n",
            "Epoch [37/50], d_loss: 1.1788, g_loss: 1.1659\n",
            "Epoch [38/50], d_loss: 0.9115, g_loss: 1.3524\n",
            "Epoch [39/50], d_loss: 0.8952, g_loss: 1.2825\n",
            "Epoch [40/50], d_loss: 0.7093, g_loss: 1.9447\n",
            "Epoch [41/50], d_loss: 0.5789, g_loss: 2.1426\n",
            "Epoch [42/50], d_loss: 0.6028, g_loss: 2.7252\n",
            "Epoch [43/50], d_loss: 0.6015, g_loss: 2.1805\n",
            "Epoch [44/50], d_loss: 0.9251, g_loss: 1.9044\n",
            "Epoch [45/50], d_loss: 0.9195, g_loss: 1.5995\n",
            "Epoch [46/50], d_loss: 1.1590, g_loss: 1.6276\n",
            "Epoch [47/50], d_loss: 0.9921, g_loss: 1.5879\n",
            "Epoch [48/50], d_loss: 1.1294, g_loss: 1.6848\n",
            "Epoch [49/50], d_loss: 0.8446, g_loss: 1.4151\n",
            "Epoch [50/50], d_loss: 1.0569, g_loss: 1.5119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKMqn2UhdIoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}